{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RECORDINGS_FOLDER = os.path.join('..', '..', 'recordings')\n",
    "\n",
    "# TODO(andrei): Trim start/end!\n",
    "RECORDINGS_TRAIN = {\n",
    "    # 'left-02.csv', \n",
    "#     'left': ['left-01.csv', 'left-02.csv'],\n",
    "#     'right': ['right-01.csv', 'right-02.csv'], #, 'right-03.csv'],\n",
    "#     'lr': ['left-01.csv', 'left-02.csv', 'right-01.csv', 'right-03.csv'],\n",
    "      'relaxed': ['closed-relax-01.csv', 'closed-relax-02.csv', 'closed-relaxed-04.csv', 'closed-relaxed-05.csv'],\n",
    "#     'forward': ['forward-01.csv'],  # TODO(andrei): Maybe 'run-01-andrei.csv'.\n",
    "#     'helicopter': ['helicopter-andrei.csv'],\n",
    "#       'baseline': ['baseline-andrei.csv', 'internet-browsing-01.csv'],\n",
    "      'tense': ['typingtest-01.csv', 'typingtest-03.csv', 'typingtest-04.csv',\n",
    "                'typingtest-05-duolingo.csv', #'typingtest-06-duolingo.csv',\n",
    "                'typingtest-07.csv']\n",
    "}\n",
    "\n",
    "RECORDINGS_VALID = {\n",
    "#     'lr': ['right-03.csv', 'left-03.csv'],\n",
    "#     'right': [],\n",
    "#     'forward': [],\n",
    "#     'helicopter': [],\n",
    "#     'baseline': ['internet-browsing-01.csv']\n",
    "    'relaxed': ['closed-relaxed-03.csv'],\n",
    "    'tense': ['typingtest-02.csv'],\n",
    "#     'baseline': []\n",
    "}\n",
    "\n",
    "\n",
    "MUSE_LABELS = {\n",
    " '/muse/acc',\n",
    " '/muse/batt',\n",
    " '/muse/config',\n",
    " '/muse/drlref',\n",
    " '/muse/eeg',\n",
    " '/muse/eeg/quantization',\n",
    " '/muse/elements/alpha_absolute',\n",
    " '/muse/elements/alpha_relative',\n",
    " '/muse/elements/alpha_session_score',\n",
    " '/muse/elements/beta_absolute',\n",
    " '/muse/elements/beta_relative',\n",
    " '/muse/elements/beta_session_score',\n",
    " '/muse/elements/blink',\n",
    " '/muse/elements/delta_absolute',\n",
    " '/muse/elements/delta_relative',\n",
    " '/muse/elements/delta_session_score',\n",
    " '/muse/elements/experimental/concentration',\n",
    " '/muse/elements/experimental/mellow',\n",
    " '/muse/elements/gamma_absolute',\n",
    " '/muse/elements/gamma_relative',\n",
    " '/muse/elements/gamma_session_score',\n",
    " '/muse/elements/horseshoe',\n",
    " '/muse/elements/is_good',\n",
    " '/muse/elements/jaw_clench',\n",
    " '/muse/elements/low_freqs_absolute',\n",
    " '/muse/elements/raw_fft0',\n",
    " '/muse/elements/raw_fft1',\n",
    " '/muse/elements/raw_fft2',\n",
    " '/muse/elements/raw_fft3',\n",
    " '/muse/elements/theta_absolute',\n",
    " '/muse/elements/theta_relative',\n",
    " '/muse/elements/theta_session_score',\n",
    " '/muse/elements/touching_forehead',\n",
    " '/muse/version'}\n",
    "\n",
    "# 4 electrodes, 4 sets of FFT coefficients per window!\n",
    "RAW_FFT0 = '/muse/elements/raw_fft0'\n",
    "RAW_FFT1 = '/muse/elements/raw_fft1'\n",
    "RAW_FFT2 = '/muse/elements/raw_fft2'\n",
    "RAW_FFT3 = '/muse/elements/raw_fft3'\n",
    "IS_GOOD = '/muse/elements/is_good'\n",
    "# interesting_feats = ['/muse/elements/raw_fft0', 'alpha_absolute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_rec(fpath: str):\n",
    "    all_ts = []\n",
    "    all_fft = []\n",
    "    all_good_masks = []\n",
    "    \n",
    "    # TODO(andrei): the IS_GOOD may contain too little info, maybe. Should try horseshoe and a thresh of like <1.25 or so.\n",
    "    \n",
    "    last_good = [0, 0, 0, 0]\n",
    "    last_good_time = -1\n",
    "    \n",
    "    current_feat = None\n",
    "    \n",
    "    # We expect fft indices always in 0-1-2-3 order. This variable keeps track of this.\n",
    "    expecting = 0\n",
    "    \n",
    "    with open(fpath, 'r') as f:\n",
    "        for line_idx, line in enumerate(f.readlines()):            \n",
    "            parts = line.split(', ')\n",
    "            ts = float(parts[0])\n",
    "            label = parts[1]\n",
    "            \n",
    "            if label.startswith('/muse/elements/raw_fft'):\n",
    "                rest_np = np.array([float(part) for part in parts[2:]])\n",
    "                idx = int(label[-1])\n",
    "                if idx != expecting:\n",
    "                    print(\"WRONG\")\n",
    "                    raise ValueError();\n",
    "                else:\n",
    "                    if current_feat is None:\n",
    "                        current_feat = rest_np\n",
    "                    else:\n",
    "                        current_feat = np.hstack((current_feat, rest_np))\n",
    "                    \n",
    "                    expecting = (idx + 1) % 4\n",
    "                    \n",
    "                    if expecting == 0:\n",
    "                        if last_good_time != -1 and (ts - last_good_time) > 0.005 and (ts - last_good_time) > 0.00:\n",
    "                            print(\"WARNING: Bad data sync.\")\n",
    "                  \n",
    "                        all_ts.append(ts)\n",
    "                        all_fft.append(current_feat)\n",
    "                        all_good_masks.append(np.all(last_good))\n",
    "#                         print('cfs', current_feat.shape)\n",
    "                        current_feat = None\n",
    "  \n",
    "            \n",
    "#             if label == RAW_FFT0:\n",
    "\n",
    "#                 all_fft.append(rest_np)\n",
    "#                 all_fft1.append(last_fft1)\n",
    "#                 all_good_masks.append(np.all(last_good))\n",
    "                  \n",
    "#                 # TODO(andrei): WARNING, this tolerance is HUGE, so may be problematic.\n",
    "#                 if last_fft1_time != -1 and (ts - last_fft1_time) > 0.18 and (ts - last_fft1_time) > 0.00:\n",
    "#                     print(\"WARNING: Bad data sync (fft1).\", ts - last_fft1_time)\n",
    "#             elif label == RAW_FFT1:\n",
    "#                 last_fft1_time = ts\n",
    "#                 rest_np = np.array([float(part) for part in parts[2:]])\n",
    "#                 if(rest_np.shape != (129,)):\n",
    "#                     print(\"WTF:\", rest_np.shape)\n",
    "#                 last_fft1 = rest_np\n",
    "            elif label == IS_GOOD:\n",
    "                last_good_time = ts\n",
    "                rest_np = np.array([float(part) for part in parts[2:]])\n",
    "                last_good = rest_np\n",
    "                \n",
    "#     print(len(all_fft))\n",
    "#     print(len(all_fft1))\n",
    "#     print(len(all_fft[0]))\n",
    "#     print(len(all_fft1[0]))\n",
    "#     print(np.array(all_fft).shape)\n",
    "#     print(np.array(all_fft1).shape)\n",
    "\n",
    "    print(len(all_fft))\n",
    "    print(np.array(all_fft).shape)\n",
    "    return np.array(all_good_masks), np.array(all_ts), np.array(all_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "1235\n",
      "(1235, 516)\n",
      "(1235, 516) (1235,)\n",
      "(1230, 516)\n",
      "(1220, 516)\n",
      "Appending to feats the following shape: (610, 1032)\n",
      "1079\n",
      "(1079, 516)\n",
      "(1079, 516) (1079,)\n",
      "(1074, 516)\n",
      "(1064, 516)\n",
      "Appending to feats the following shape: (532, 1032)\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "1445\n",
      "(1445, 516)\n",
      "(1445, 516) (1445,)\n",
      "(1433, 516)\n",
      "(1423, 516)\n",
      "Appending to feats the following shape: (711, 1032)\n",
      "WARNING: Bad data sync.\n",
      "1074\n",
      "(1074, 516)\n",
      "(1074, 516) (1074,)\n",
      "(1064, 516)\n",
      "(1054, 516)\n",
      "Appending to feats the following shape: (527, 1032)\n",
      "1368\n",
      "(1368, 516)\n",
      "(1368, 516) (1368,)\n",
      "(1182, 516)\n",
      "(1172, 516)\n",
      "Appending to feats the following shape: (586, 1032)\n",
      "1895\n",
      "(1895, 516)\n",
      "(1895, 516) (1895,)\n",
      "(45, 516)\n",
      "(35, 516)\n",
      "Appending to feats the following shape: (17, 1032)\n",
      "1997\n",
      "(1997, 516)\n",
      "(1997, 516) (1997,)\n",
      "(1923, 516)\n",
      "(1913, 516)\n",
      "Appending to feats the following shape: (956, 1032)\n",
      "WARNING: Bad data sync.\n",
      "1201\n",
      "(1201, 516)\n",
      "(1201, 516) (1201,)\n",
      "(887, 516)\n",
      "(877, 516)\n",
      "Appending to feats the following shape: (438, 1032)\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "1899\n",
      "(1899, 516)\n",
      "(1899, 516) (1899,)\n",
      "(1846, 516)\n",
      "(1836, 516)\n",
      "Appending to feats the following shape: (918, 1032)\n",
      "relaxed: (2380, 1032)\n",
      "tense: (2915, 1032)\n",
      "Processing validation...\n",
      "1455\n",
      "(1455, 516)\n",
      "(1455, 516) (1455,)\n",
      "(948, 516)\n",
      "(938, 516)\n",
      "Appending to feats the following shape: (469, 1032)\n",
      "1881\n",
      "(1881, 516)\n",
      "(1881, 516) (1881,)\n",
      "(1696, 516)\n",
      "(1686, 516)\n",
      "Appending to feats the following shape: (843, 1032)\n",
      "relaxed: (469, 1032)\n",
      "tense: (843, 1032)\n"
     ]
    }
   ],
   "source": [
    "def spec2data(recording_map):\n",
    "    data_map = {}\n",
    "    for label, fnames in recording_map.items():\n",
    "        cfeats = []\n",
    "        for fname in fnames:\n",
    "            # Read the data for that recording for that \n",
    "            good_mask, all_ts, all_feats = read_rec(os.path.join(RECORDINGS_FOLDER, fname))\n",
    "    #         all_ts = all_ts[good_mask]\n",
    "            print(all_feats.shape, good_mask.shape)\n",
    "            # Remove some edge frames, since they can be noisy due to the nature of the experiments.\n",
    "            EDGE_CUTOFF = 10\n",
    "            all_feats = all_feats[good_mask, :]\n",
    "            print(all_feats.shape)\n",
    "            all_feats = all_feats[EDGE_CUTOFF:, :]\n",
    "            print(all_feats.shape)\n",
    "            \n",
    "            # Clump up all k consecutive entries into a single one (poor man's CNNs).\n",
    "            FACTOR = 2\n",
    "            # Need to shave off a little at the end.\n",
    "            shave_right = all_feats.shape[0] % FACTOR\n",
    "            if shave_right > 0:\n",
    "                all_feats = all_feats[:-shave_right, :]\n",
    "            all_feats = all_feats.reshape(-1, all_feats.shape[1] * FACTOR) #, order='C')\n",
    "           \n",
    "            print(\"Appending to feats the following shape:\", all_feats.shape)\n",
    "            cfeats.append(all_feats)\n",
    "\n",
    "        if len(cfeats) > 1:\n",
    "            data_map[label] = np.vstack(cfeats)\n",
    "        elif len(cfeats) == 1:\n",
    "            data_map[label] = cfeats[0]\n",
    "        else:\n",
    "            data_map[label] = np.array([])\n",
    "    \n",
    "    for label, data in data_map.items():\n",
    "        print(\"{0}: {1}\".format(label, data.shape))\n",
    "        \n",
    "    return data_map\n",
    "\n",
    "print(\"Processing train...\")\n",
    "data_map_train = spec2data(RECORDINGS_TRAIN)\n",
    "print(\"Processing validation...\")\n",
    "data_map_test = spec2data(RECORDINGS_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 relaxed\n",
      "1 tense\n",
      "0 relaxed\n",
      "1 tense\n"
     ]
    }
   ],
   "source": [
    "def gen_data_matrix(data_map):\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "    idx2label = {}\n",
    "    for idx, (label, data) in enumerate(data_map.items()):\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "            \n",
    "        idx2label[idx] = label\n",
    "        print(idx, label)\n",
    "            \n",
    "        if X is None:\n",
    "            X = data\n",
    "            y = np.ones(X.shape[0]) * idx\n",
    "        else:\n",
    "            X = np.vstack((X, data))\n",
    "            y = np.hstack((y, np.ones(data.shape[0]) * idx))\n",
    "\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n",
    "    return X, y, idx2label\n",
    "    \n",
    "X_train, y_train, idx2label = gen_data_matrix(data_map_train)\n",
    "X_test, y_test, _ = gen_data_matrix(data_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(\", \".join([str(d) for d in X_train[0]]))\n",
    "\n",
    "# all_ts_run = np.array(all_ts_run)\n",
    "# deltas = all_ts_run[1:] - all_ts_run[:-1]\n",
    "# print(deltas.mean())\n",
    "# print(np.median(deltas))\n",
    "# print(deltas.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5295, 1032) (5295,)\n",
      "float64 float64\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.vstack((all_fft_base, all_fft_run))\n",
    "# y_train = np.hstack((np.zeros(all_fft_base.shape[0]), np.ones(all_fft_run.shape[0])))\n",
    "# X_train = np.vstack((all_fft_heli, all_fft_run, all_fft_base))\n",
    "# y_train = np.hstack((np.zeros(all_fft_heli.shape[0]), np.ones(all_fft_run.shape[0]), 2 * np.ones(all_fft_base.shape[0])))\n",
    "\n",
    "# print(all_fft_base.shape, all_fft_run.shape, all_fft_heli.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train.dtype, y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"{2}: Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores),\n",
    "              i + 1))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2380    0    0    0    0    0    0    0    0 2915]\n",
      "(5295, 1032)\n",
      "Kicking off grid search...\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:   19.5s\n",
      "[Parallel(n_jobs=-2)]: Done  50 out of  50 | elapsed:   28.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import *\n",
    "\n",
    "# clf = DummyClassifier()\n",
    "# clf = RandomForestClassifier()\n",
    "# clf = AdaBoostClassifier()\n",
    "# clf = SVC()\n",
    "# clf = SGDClassifier()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "Cs = [0.01, 0.1, 1.0, 2.0, 10, 25]\n",
    "# tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "#                      'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 25, 50]},\n",
    "#                     {'kernel': ['linear'], 'C': [0.001, 0.01, 0.05, 0.1, 0.5, 10, 50, 100]}]\n",
    "# cws = ['balanced', {0: 3, 1: 1}, {0: 1, 1: 1}]\n",
    "tuned_parameters = [ # SVC\n",
    "    {\n",
    "        'clf__kernel': ['linear'],\n",
    "        'clf__C': Cs,\n",
    "    },\n",
    "#     {\n",
    "#         'clf__kernel': ['rbf'],\n",
    "#         'clf__gamma': [0.1, 0.01, 0.001], #, 0.0001],\n",
    "#         'clf__C': Cs,\n",
    "#         'epsilon': []\n",
    "#     },\n",
    "#     {\n",
    "# #         'kernel': ['poly']\n",
    "#     }\n",
    "]\n",
    "tuned_parameters = {\n",
    "#     'clf__n_estimators': [2, 3, 5, 10, 15], #, 15, 20], #, 25, 50], #, 75, 100, 125]#, 150],\n",
    "      'clf__n_estimators': [10, 50, 100],\n",
    "#     'clf__class_weight': ['balanced', None],\n",
    "#     'scaler__with_mean': [True, False],\n",
    "#     'scaler__with_std': [True, False],\n",
    "}\n",
    "\n",
    "tuned_parameters = { # LogReg\n",
    "    # C = Inverse of regularization strength; must be a positive float. \n",
    "    # Like in support vector machines, smaller values specify stronger regularization.\n",
    "#     'clf__C': [0.01, 0.1, 0.25, 1.0],\n",
    "    'clf__C': [0.0005, 0.001, 0.005, 0.01, 0.05]#, 0.1, 0.25]#, 1.0, 5.0, 10.0]\n",
    "#     'scaler__with_mean': [True, False],\n",
    "#     'scaler__with_std': [True, False],\n",
    "}\n",
    "\n",
    "# tuned_parameters = {  # SGDClassifier\n",
    "# #     'clf__epsilon' : [0.01, 0.1, 1.0],\n",
    "# #     'clf__alpha': [0.05, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "#     'clf__alpha': [2.5, 5.0, 10.0, 15.0],\n",
    "#     'clf__class_weight': ['balanced', None],\n",
    "# }\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "print(np.histogram(y_train)[0])\n",
    "grid = GridSearchCV(pipeline, tuned_parameters, cv=10, scoring='f1_micro', n_jobs=-2, verbose=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Kicking off grid search...\")\n",
    "\n",
    "# Hack to suppress tons of output when doing feature selection (doesn't impact the actual feature selection)\n",
    "# np.seterr(all='ignore')\n",
    "res = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mean validation score: 0.774 (std: 0.044)\n",
      "Parameters: {'clf__C': 0.001}\n",
      "2: Mean validation score: 0.774 (std: 0.040)\n",
      "Parameters: {'clf__C': 0.005}\n",
      "3: Mean validation score: 0.772 (std: 0.049)\n",
      "Parameters: {'clf__C': 0.0005}\n",
      "4: Mean validation score: 0.768 (std: 0.042)\n",
      "Parameters: {'clf__C': 0.01}\n",
      "5: Mean validation score: 0.764 (std: 0.046)\n",
      "Parameters: {'clf__C': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrei/anaconda3/envs/oxh/lib/python3.5/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "report(res.grid_scores_, n_top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'relaxed', 1: 'tense'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2272,  108],\n",
       "       [ 270, 2645]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = grid.predict(X_train)\n",
    "print(idx2label)\n",
    "confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[279 190] [338 505]\n",
      "0 1\n",
      "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "yy=res.predict(X_test)\n",
    "# print(np.bincount(y_test.astype(np.int64)))\n",
    "# print(yy)\n",
    "print(y_test[0:10], y_test[-10:])\n",
    "counts1 = np.bincount(yy[:469].astype(np.int64))\n",
    "counts2 = np.bincount(yy[469:].astype(np.int64))\n",
    "print(counts1, counts2)\n",
    "print(np.argmax(counts1), np.argmax(counts2))\n",
    "print(res.best_estimator_)\n",
    "# print(f1_score(y_test, yy, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LogReg: [446 466 250]\n",
    "# LinSVC: [614 393 155] -> overfits\n",
    "# SGDClassifier: [614 393 155] -> overfits (same as LinSVC; just faster)\n",
    "# RFC: [481 340 341], [597 372 193] \n",
    "# RFC, different case: [738 744 194], [676 716 284] (middle one is correct)\n",
    "\n",
    "# stacking every 2 consecutive feature vectors: \n",
    "#  - [383 412  48] with LogReg (best, so far)\n",
    "#  - [354 338 151] with RFC\n",
    "#  - [456 372  15] with SVC\n",
    "#  - [208 594  41] with AdaBoost (100 estimators)\n",
    "\n",
    "# Added way more data (e.g. the 2 duolingo sessions)\n",
    "#  - [  0 199 270] [ 42 108 693] == [2, 2] with AdaBoost (not ignoring baseline; under-represented)\n",
    "#  - <bad> with SGDClassifier (no more baseline class)\n",
    "#  - [258 211] [273 570] == [0, 1] with LogReg\n",
    "#  - [257 212] [279 564] == [0, 1] with LogReg, more regularization\n",
    "#  - [279 190] [338 505] == [0, 1] with LogReg, even more regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dumped model pickle.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../model-lr.pkl', 'wb') as f:\n",
    "    pickle.dump(res.best_estimator_, f)\n",
    "    print(\"Successfully dumped model pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
