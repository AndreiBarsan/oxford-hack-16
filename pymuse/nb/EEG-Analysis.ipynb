{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RECORDINGS_FOLDER = os.path.join('..', '..', 'recordings')\n",
    "\n",
    "RECORDINGS_TRAIN = {\n",
    "    # 'left-02.csv', \n",
    "#     'left': ['left-01.csv', 'left-02.csv'],\n",
    "#     'right': ['right-01.csv', 'right-02.csv'], #, 'right-03.csv'],\n",
    "#     'lr': ['left-01.csv', 'left-02.csv', 'right-01.csv', 'right-03.csv'],\n",
    "      'relaxed': ['closed-relax-01.csv', 'closed-relax-02.csv'],\n",
    "#     'forward': ['forward-01.csv'],  # TODO(andrei): Maybe 'run-01-andrei.csv'.\n",
    "#     'helicopter': ['helicopter-andrei.csv'],\n",
    "      'baseline': ['baseline-andrei.csv', 'internet-browsing-01.csv'],\n",
    "      'tense': ['typingtest-01.csv', 'typingtest-03.csv']\n",
    "}\n",
    "\n",
    "RECORDINGS_VALID = {\n",
    "#     'lr': ['right-03.csv', 'left-03.csv'],\n",
    "#     'right': [],\n",
    "#     'forward': [],\n",
    "#     'helicopter': [],\n",
    "#     'baseline': ['internet-browsing-01.csv']\n",
    "    'relaxed': [],\n",
    "    'tense': ['typingtest-02.csv'],\n",
    "    'baseline': []\n",
    "}\n",
    "\n",
    "\n",
    "MUSE_LABELS = {\n",
    " '/muse/acc',\n",
    " '/muse/batt',\n",
    " '/muse/config',\n",
    " '/muse/drlref',\n",
    " '/muse/eeg',\n",
    " '/muse/eeg/quantization',\n",
    " '/muse/elements/alpha_absolute',\n",
    " '/muse/elements/alpha_relative',\n",
    " '/muse/elements/alpha_session_score',\n",
    " '/muse/elements/beta_absolute',\n",
    " '/muse/elements/beta_relative',\n",
    " '/muse/elements/beta_session_score',\n",
    " '/muse/elements/blink',\n",
    " '/muse/elements/delta_absolute',\n",
    " '/muse/elements/delta_relative',\n",
    " '/muse/elements/delta_session_score',\n",
    " '/muse/elements/experimental/concentration',\n",
    " '/muse/elements/experimental/mellow',\n",
    " '/muse/elements/gamma_absolute',\n",
    " '/muse/elements/gamma_relative',\n",
    " '/muse/elements/gamma_session_score',\n",
    " '/muse/elements/horseshoe',\n",
    " '/muse/elements/is_good',\n",
    " '/muse/elements/jaw_clench',\n",
    " '/muse/elements/low_freqs_absolute',\n",
    " '/muse/elements/raw_fft0',\n",
    " '/muse/elements/raw_fft1',\n",
    " '/muse/elements/raw_fft2',\n",
    " '/muse/elements/raw_fft3',\n",
    " '/muse/elements/theta_absolute',\n",
    " '/muse/elements/theta_relative',\n",
    " '/muse/elements/theta_session_score',\n",
    " '/muse/elements/touching_forehead',\n",
    " '/muse/version'}\n",
    "\n",
    "# 4 electrodes, 4 sets of FFT coefficients per window!\n",
    "RAW_FFT0 = '/muse/elements/raw_fft0'\n",
    "RAW_FFT1 = '/muse/elements/raw_fft1'\n",
    "RAW_FFT2 = '/muse/elements/raw_fft2'\n",
    "RAW_FFT3 = '/muse/elements/raw_fft3'\n",
    "IS_GOOD = '/muse/elements/is_good'\n",
    "# interesting_feats = ['/muse/elements/raw_fft0', 'alpha_absolute']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_rec(fpath: str):\n",
    "    all_ts = []\n",
    "    all_fft = []\n",
    "    all_good_masks = []\n",
    "    \n",
    "    # TODO(andrei): the IS_GOOD may contain too little info, maybe. Should try horseshoe and a thresh of like <1.25 or so.\n",
    "    \n",
    "    last_good = [0, 0, 0, 0]\n",
    "    last_good_time = -1\n",
    "    \n",
    "    current_feat = None\n",
    "    \n",
    "    # We expect fft indices always in 0-1-2-3 order. This variable keeps track of this.\n",
    "    expecting = 0\n",
    "    \n",
    "    with open(fpath, 'r') as f:\n",
    "        for line_idx, line in enumerate(f.readlines()):            \n",
    "            parts = line.split(', ')\n",
    "            ts = float(parts[0])\n",
    "            label = parts[1]\n",
    "            \n",
    "            if label.startswith('/muse/elements/raw_fft'):\n",
    "                rest_np = np.array([float(part) for part in parts[2:]])\n",
    "                idx = int(label[-1])\n",
    "                if idx != expecting:\n",
    "                    print(\"WRONG\")\n",
    "                    raise ValueError();\n",
    "                else:\n",
    "                    if current_feat is None:\n",
    "                        current_feat = rest_np\n",
    "                    else:\n",
    "                        current_feat = np.hstack((current_feat, rest_np))\n",
    "                    \n",
    "                    expecting = (idx + 1) % 4\n",
    "                    \n",
    "                    if expecting == 0:\n",
    "                        if last_good_time != -1 and (ts - last_good_time) > 0.005 and (ts - last_good_time) > 0.00:\n",
    "                            print(\"WARNING: Bad data sync.\")\n",
    "                  \n",
    "                        all_ts.append(ts)\n",
    "                        all_fft.append(current_feat)\n",
    "                        all_good_masks.append(np.all(last_good))\n",
    "#                         print('cfs', current_feat.shape)\n",
    "                        current_feat = None\n",
    "  \n",
    "            \n",
    "#             if label == RAW_FFT0:\n",
    "\n",
    "#                 all_fft.append(rest_np)\n",
    "#                 all_fft1.append(last_fft1)\n",
    "#                 all_good_masks.append(np.all(last_good))\n",
    "                  \n",
    "#                 # TODO(andrei): WARNING, this tolerance is HUGE, so may be problematic.\n",
    "#                 if last_fft1_time != -1 and (ts - last_fft1_time) > 0.18 and (ts - last_fft1_time) > 0.00:\n",
    "#                     print(\"WARNING: Bad data sync (fft1).\", ts - last_fft1_time)\n",
    "#             elif label == RAW_FFT1:\n",
    "#                 last_fft1_time = ts\n",
    "#                 rest_np = np.array([float(part) for part in parts[2:]])\n",
    "#                 if(rest_np.shape != (129,)):\n",
    "#                     print(\"WTF:\", rest_np.shape)\n",
    "#                 last_fft1 = rest_np\n",
    "            elif label == IS_GOOD:\n",
    "                last_good_time = ts\n",
    "                rest_np = np.array([float(part) for part in parts[2:]])\n",
    "                last_good = rest_np\n",
    "                \n",
    "#     print(len(all_fft))\n",
    "#     print(len(all_fft1))\n",
    "#     print(len(all_fft[0]))\n",
    "#     print(len(all_fft1[0]))\n",
    "#     print(np.array(all_fft).shape)\n",
    "#     print(np.array(all_fft1).shape)\n",
    "\n",
    "    print(len(all_fft))\n",
    "    print(np.array(all_fft).shape)\n",
    "    return np.array(all_good_masks), np.array(all_ts), np.array(all_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "1368\n",
      "(1368, 516)\n",
      "(1368, 516) (1368,)\n",
      "1881\n",
      "(1881, 516)\n",
      "(1881, 516) (1881,)\n",
      "1235\n",
      "(1235, 516)\n",
      "(1235, 516) (1235,)\n",
      "1079\n",
      "(1079, 516)\n",
      "(1079, 516) (1079,)\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "1806\n",
      "(1806, 516)\n",
      "(1806, 516) (1806,)\n",
      "WARNING: Bad data sync.\n",
      "WARNING: Bad data sync.\n",
      "1996\n",
      "(1996, 516)\n",
      "(1996, 516) (1996,)\n",
      "tense: (2878, 516)\n",
      "relaxed: (2304, 516)\n",
      "baseline: (2764, 516)\n",
      "Processing validation...\n",
      "1895\n",
      "(1895, 516)\n",
      "(1895, 516) (1895,)\n",
      "tense: (45, 516)\n",
      "relaxed: (0,)\n",
      "baseline: (0,)\n"
     ]
    }
   ],
   "source": [
    "def spec2data(recording_map):\n",
    "    data_map = {}\n",
    "    for label, fnames in recording_map.items():\n",
    "        cfeats = []\n",
    "        for fname in fnames:\n",
    "            # Read the data for that recording for that \n",
    "            good_mask, all_ts, all_feats = read_rec(os.path.join(RECORDINGS_FOLDER, fname))\n",
    "    #         all_ts = all_ts[good_mask]\n",
    "            print(all_feats.shape, good_mask.shape)\n",
    "            all_feats = all_feats[good_mask]\n",
    "            cfeats.append(all_feats)\n",
    "\n",
    "        if len(cfeats) > 1:\n",
    "            data_map[label] = np.vstack(cfeats)\n",
    "        elif len(cfeats) == 1:\n",
    "            data_map[label] = cfeats[0]\n",
    "        else:\n",
    "            data_map[label] = np.array([])\n",
    "    \n",
    "    for label, data in data_map.items():\n",
    "        print(\"{0}: {1}\".format(label, data.shape))\n",
    "        \n",
    "    return data_map\n",
    "\n",
    "print(\"Processing train...\")\n",
    "data_map_train = spec2data(RECORDINGS_TRAIN)\n",
    "print(\"Processing validation...\")\n",
    "data_map_test = spec2data(RECORDINGS_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tense\n",
      "1 relaxed\n",
      "2 baseline\n",
      "0 tense\n"
     ]
    }
   ],
   "source": [
    "def gen_data_matrix(data_map):\n",
    "    X = None\n",
    "    y = None\n",
    "\n",
    "    for idx, (label, data) in enumerate(data_map.items()):\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "            \n",
    "        print(idx, label)\n",
    "            \n",
    "        if X is None:\n",
    "            X = data\n",
    "            y = np.zeros(X.shape[0])\n",
    "        else:\n",
    "            X = np.vstack((X, data))\n",
    "            y = np.hstack((y, np.ones(data.shape[0]) * idx))\n",
    "\n",
    "#     print(X.shape)\n",
    "#     print(y.shape)\n",
    "    return X, y\n",
    "    \n",
    "X_train, y_train = gen_data_matrix(data_map_train)\n",
    "X_test, y_test = gen_data_matrix(data_map_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.9511175, 18.291094, 20.788092, 22.603186, 23.209288, 22.306587, 19.845474, 15.335599, 13.440422, 14.134462, 13.811703, 12.711906, 8.919536, 0.3454846, 0.6849613, 6.1878805, 6.5826874, 7.902959, 5.829215, 0.29376704, -4.789386, -5.8287697, -3.3021586, -2.1122236, -1.4895154, -0.8462546, -10.879423, -6.6052256, 0.17633395, -0.9099651, -3.7232182, -8.046315, -7.6615515, -1.0993654, -2.224989, -5.712921, -4.9520135, -3.4098516, -7.8462725, -4.8222146, -6.8093023, -14.508396, -10.934134, -8.291086, -7.6863613, -4.1332884, -5.0380807, -12.109217, -9.899872, -4.034348, -7.2110343, -3.2156935, -2.554939, -7.32338, -4.051792, -2.977265, -5.8236527, -9.683343, -12.700991, -12.114233, -9.5633545, -10.19207, -11.097839, -11.437096, -13.15785, -10.935534, -9.01671, -14.331649, -25.754673, -29.391386, -27.281206, -20.154917, -21.09897, -22.342802, -17.841003, -12.930537, -14.874968, -11.776426, -10.409021, -9.799373, -11.961889, -12.736766, -13.440732, -10.562811, -10.9712, -6.95038, -9.346109, -10.297009, -9.570436, -9.498728, -6.5635347, -10.744911, -9.384317, -10.740901, -10.178785, -15.414688, -8.772016, -11.7697315, -12.060352, -13.464639, -12.114177, -11.253095, -9.801867, -12.075151, -13.824127, -11.965523, -14.037358, -13.294287, -10.451481, -11.019818, -13.497123, -8.971289, -8.453857, -8.2756195, -14.339793, -13.907322, -12.373324, -17.331152, -18.939043, -18.017817, -14.89738, -17.168938, -16.830114, -14.753596, -13.440431, -18.1467, -12.1263075, -15.792633, -20.802994, 2.586489, 7.537117, 8.249563, 8.841319, 9.156076, 8.094372, 5.851574, 0.47810483, -2.784396, -0.026125686, -0.9687587, -3.5190623, -6.9440637, -9.725869, -13.947346, -9.669693, -7.897372, -11.880797, -12.148529, -16.590155, -20.563538, -20.970276, -20.301527, -19.338383, -14.178545, -13.1782055, -17.287792, -22.634695, -19.255585, -14.508697, -13.371112, -16.284529, -21.484287, -21.99339, -15.7166815, -17.734722, -17.938557, -15.380808, -20.729427, -21.04313, -20.918013, -15.266011, -14.323181, -15.856671, -18.385338, -19.612352, -21.017847, -23.856243, -17.470552, -11.991487, -13.171358, -13.486578, -11.797061, -14.837313, -15.186437, -18.753607, -18.266214, -21.967382, -21.601831, -23.628946, -22.903769, -25.800577, -24.149126, -21.529516, -21.26117, -23.324705, -24.330748, -23.803276, -29.464071, -34.082573, -35.79709, -33.43429, -24.5389, -23.420088, -25.205873, -20.132908, -19.909018, -24.847279, -19.309841, -19.012901, -20.734999, -21.716503, -23.16608, -21.821733, -23.046637, -20.16694, -22.056717, -23.716137, -19.63033, -22.553373, -23.258945, -18.706526, -17.950281, -20.476116, -20.425753, -20.974287, -21.749517, -23.081757, -25.404823, -24.570536, -29.083483, -24.849611, -21.20695, -24.21177, -20.972252, -21.43552, -27.528051, -23.758408, -25.971426, -23.064095, -22.896078, -22.148357, -23.083696, -23.86401, -24.895815, -22.757553, -22.119131, -24.358435, -29.511753, -33.52714, -27.426193, -26.280056, -31.556221, -29.413801, -28.719069, -27.909485, -28.96008, -25.280499, -29.840214, -1.6856616, 5.644464, 8.79743, 6.0341554, -1.636904, -3.38235, -3.515958, -9.034155, -9.109829, -6.7481065, -9.276532, -5.014966, -8.661515, -13.607794, -19.252768, -17.281681, -12.731165, -11.726681, -9.223967, -5.6275034, -7.9876814, -14.054981, -21.103037, -11.12213, -13.665533, -17.511665, -18.661854, -16.152376, -18.441553, -20.519484, -15.544797, -17.0143, -14.278704, -15.120726, -14.179535, -16.019625, -14.893338, -15.410744, -18.07025, -17.442324, -19.12917, -20.144148, -20.871717, -17.026482, -20.438997, -20.27694, -21.22679, -17.806898, -21.217142, -24.869974, -20.138945, -16.803127, -14.859582, -19.40893, -20.960142, -17.814867, -19.08596, -19.653578, -17.39989, -14.227726, -18.050423, -22.18065, -23.709068, -23.677242, -18.840912, -21.813053, -24.227007, -27.969398, -31.871464, -36.119167, -41.640484, -34.355038, -33.663166, -22.620825, -23.243704, -22.532774, -21.343498, -23.462088, -21.657318, -22.480516, -21.284422, -21.763105, -22.960543, -19.923658, -20.289225, -25.520569, -23.257656, -21.715763, -18.135416, -22.190948, -26.137772, -24.641897, -19.614538, -19.736591, -20.572247, -20.11603, -24.81629, -21.412756, -16.459597, -18.391636, -18.974646, -24.51461, -30.243078, -24.312544, -20.353786, -23.320988, -26.231499, -25.00089, -23.238338, -21.965223, -24.80989, -31.813265, -26.352598, -26.142212, -27.866734, -22.607506, -26.179935, -25.043577, -24.81627, -29.404345, -29.025454, -26.761038, -27.301907, -28.621466, -28.003304, -28.023432, -28.022581, -28.527802, -29.089787, 12.304302, 18.479618, 21.163597, 23.323349, 23.588722, 22.771286, 20.241627, 15.415336, 13.654505, 14.82232, 14.628935, 13.944979, 10.687833, 4.2788205, -3.7876813, 0.7392125, 5.5071535, 5.623304, 2.255998, 0.06325606, -0.41648805, -3.1690722, -4.9696546, -4.553699, -7.6599345, -8.994194, -10.203782, -11.804766, -13.883099, -9.4699135, -9.456641, -11.569683, -14.454867, -8.219205, -3.6478612, -5.9166603, -14.857715, -13.21508, -6.508441, -7.1028976, -8.583575, -6.402012, -4.16153, -5.1079073, -7.860279, -11.885599, -11.736027, -10.204596, -10.569895, -12.30068, -12.089546, -11.2668915, -11.705677, -9.21771, -10.43442, -13.415394, -6.91431, -10.875048, -16.262989, -15.9520035, -21.020502, -19.844662, -13.234308, -15.506639, -19.971075, -17.3615, -20.541817, -20.42122, -21.862728, -26.557213, -29.042498, -28.658943, -24.66576, -21.902016, -21.34855, -20.860626, -14.81127, -12.164649, -12.073309, -12.047047, -11.831438, -8.732765, -8.69849, -7.0620594, -7.722377, -15.159487, -13.618369, -11.149188, -12.394432, -11.331817, -17.649448, -18.23439, -16.750818, -11.323679, -13.639443, -13.318474, -14.150101, -17.371872, -24.12098, -14.537436, -14.544449, -17.424685, -14.519746, -18.182201, -17.31756, -16.425207, -15.791033, -10.226565, -11.408097, -16.188871, -17.509943, -12.853909, -15.701295, -16.025898, -15.058625, -17.443087, -20.414883, -24.867283, -25.778328, -22.955643, -17.51347, -20.119318, -20.294922, -21.556606, -23.922796, -21.618675, -18.853333, -22.970161, -20.411488\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join([str(d) for d in X_train[0]]))\n",
    "\n",
    "# all_ts_run = np.array(all_ts_run)\n",
    "# deltas = all_ts_run[1:] - all_ts_run[:-1]\n",
    "# print(deltas.mean())\n",
    "# print(np.median(deltas))\n",
    "# print(deltas.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7946, 516) (7946,)\n",
      "float64 float64\n"
     ]
    }
   ],
   "source": [
    "# X_train = np.vstack((all_fft_base, all_fft_run))\n",
    "# y_train = np.hstack((np.zeros(all_fft_base.shape[0]), np.ones(all_fft_run.shape[0])))\n",
    "# X_train = np.vstack((all_fft_heli, all_fft_run, all_fft_base))\n",
    "# y_train = np.hstack((np.zeros(all_fft_heli.shape[0]), np.ones(all_fft_run.shape[0]), 2 * np.ones(all_fft_base.shape[0])))\n",
    "\n",
    "# print(all_fft_base.shape, all_fft_run.shape, all_fft_heli.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_train.dtype, y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report(grid_scores, n_top=3):\n",
    "    top_scores = sorted(grid_scores, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"{2}: Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores),\n",
    "              i + 1))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2878    0    0    0    0 2304    0    0    0 2764]\n",
      "(7946, 516)\n",
      "Kicking off grid search...\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Done  36 tasks      | elapsed:   59.5s\n",
      "[Parallel(n_jobs=-2)]: Done  60 out of  60 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.tree import *\n",
    "from sklearn.neighbors import *\n",
    "from sklearn.pipeline import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import *\n",
    "\n",
    "# clf = DummyClassifier()\n",
    "clf = RandomForestClassifier()\n",
    "# clf = SVC()\n",
    "clf = LogisticRegression()\n",
    "\n",
    "Cs = [0.01, 0.1, 1.0, 2.0, 10, 25]\n",
    "# tuned_parameters = [{'kernel': ['rbf'], 'gamma': [0.1, 0.01, 0.001, 0.0001],\n",
    "#                      'C': [0.001, 0.01, 0.1, 0.5, 1, 5, 10, 25, 50]},\n",
    "#                     {'kernel': ['linear'], 'C': [0.001, 0.01, 0.05, 0.1, 0.5, 10, 50, 100]}]\n",
    "# cws = ['balanced', {0: 3, 1: 1}, {0: 1, 1: 1}]\n",
    "tuned_parameters = [ # SVC\n",
    "#     {\n",
    "#         'clf__kernel': ['linear'],\n",
    "#         'clf__C': Cs,\n",
    "#     },\n",
    "    {\n",
    "        'clf__kernel': ['rbf'],\n",
    "        'clf__gamma': [0.1, 0.01, 0.001], #, 0.0001],\n",
    "        'clf__C': Cs,\n",
    "#         'epsilon': []\n",
    "    },\n",
    "#     {\n",
    "# #         'kernel': ['poly']\n",
    "#     }\n",
    "]\n",
    "tuned_parameters = {\n",
    "    'clf__n_estimators': [2, 3, 5, 10], #, 15, 20], #, 25, 50], #, 75, 100, 125]#, 150],\n",
    "    'clf__class_weight': ['balanced'],\n",
    "#     'scaler__with_mean': [True, False],\n",
    "#     'scaler__with_std': [True, False],\n",
    "}\n",
    "\n",
    "tuned_parameters = { # LogReg\n",
    "    'clf__C': [0.01, 0.1, 0.25, 1.0, 2.5, 5.0],\n",
    "#     'scaler__with_mean': [True, False],\n",
    "#     'scaler__with_std': [True, False],\n",
    "}\n",
    "# tuned_parameters = { }\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "print(np.histogram(y_train)[0])\n",
    "grid = GridSearchCV(pipeline, tuned_parameters, cv=10, scoring='f1_macro', n_jobs=-2, verbose=1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Kicking off grid search...\")\n",
    "\n",
    "# Hack to suppress tons of output when doing feature selection (doesn't impact the actual feature selection)\n",
    "# np.seterr(all='ignore')\n",
    "res = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Mean validation score: 0.881 (std: 0.050)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.001, 'scaler__with_mean': True}\n",
      "2: Mean validation score: 0.880 (std: 0.040)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.005, 'scaler__with_mean': True}\n",
      "3: Mean validation score: 0.880 (std: 0.046)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.001, 'scaler__with_mean': False}\n",
      "4: Mean validation score: 0.878 (std: 0.039)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.01, 'scaler__with_mean': True}\n",
      "5: Mean validation score: 0.872 (std: 0.044)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.005, 'scaler__with_mean': False}\n",
      "6: Mean validation score: 0.868 (std: 0.036)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.1, 'scaler__with_mean': True}\n",
      "7: Mean validation score: 0.867 (std: 0.041)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.01, 'scaler__with_mean': False}\n",
      "8: Mean validation score: 0.866 (std: 0.037)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.001, 'scaler__with_mean': True}\n",
      "9: Mean validation score: 0.865 (std: 0.034)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.25, 'scaler__with_mean': True}\n",
      "10: Mean validation score: 0.861 (std: 0.044)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.001, 'scaler__with_mean': False}\n",
      "11: Mean validation score: 0.859 (std: 0.039)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.005, 'scaler__with_mean': True}\n",
      "12: Mean validation score: 0.857 (std: 0.039)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.01, 'scaler__with_mean': True}\n",
      "13: Mean validation score: 0.857 (std: 0.044)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.005, 'scaler__with_mean': False}\n",
      "14: Mean validation score: 0.857 (std: 0.041)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.1, 'scaler__with_mean': False}\n",
      "15: Mean validation score: 0.854 (std: 0.044)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.01, 'scaler__with_mean': False}\n",
      "16: Mean validation score: 0.853 (std: 0.039)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.1, 'scaler__with_mean': True}\n",
      "17: Mean validation score: 0.852 (std: 0.038)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.25, 'scaler__with_mean': True}\n",
      "18: Mean validation score: 0.852 (std: 0.041)\n",
      "Parameters: {'scaler__with_std': True, 'clf__C': 0.25, 'scaler__with_mean': False}\n",
      "19: Mean validation score: 0.847 (std: 0.045)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.1, 'scaler__with_mean': False}\n",
      "20: Mean validation score: 0.846 (std: 0.046)\n",
      "Parameters: {'scaler__with_std': False, 'clf__C': 0.25, 'scaler__with_mean': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrei/anaconda3/envs/oxh/lib/python3.5/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "report(res.grid_scores_, n_top=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2803,   48,   27],\n",
       "       [  26, 2278,    0],\n",
       "       [   0,   24, 2740]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_train = grid.predict(X_train)\n",
    "confusion_matrix(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  2.  2.]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "yy=res.predict(X_test)\n",
    "print(yy)\n",
    "counts = np.bincount(yy.astype(np.int64))\n",
    "print(np.argmax(counts))\n",
    "# print(f1_score(y_test, yy, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=10, error_score='raise',\n",
      "       estimator=Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('clf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False))]),\n",
      "       fit_params={}, iid=True, n_jobs=-2,\n",
      "       param_grid={'clf__n_estimators': [10, 15, 20, 25, 50], 'clf__class_weight': ['balanced']},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
      "       scoring='f1_micro', verbose=1)\n"
     ]
    }
   ],
   "source": [
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dumped model pickle.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../model-lr.pkl', 'wb') as f:\n",
    "    pickle.dump(res.best_estimator_, f)\n",
    "    print(\"Successfully dumped model pickle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
